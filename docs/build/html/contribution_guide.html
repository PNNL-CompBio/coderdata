<!DOCTYPE html>

<html lang="en" data-content_root="./">
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="viewport" content="width=device-width, initial-scale=1" />

    <title>Contribution &#8212; CoderData</title>
    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=d75fae25" />
    <link rel="stylesheet" type="text/css" href="_static/sphinxdoc.css?v=34905f61" />
    <link rel="stylesheet" type="text/css" href="_static/custom.css?v=6e3806b9" />
    <script src="_static/documentation_options.js?v=8d563738"></script>
    <script src="_static/doctools.js?v=9bcbadda"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script crossorigin="anonymous" integrity="sha256-Ae2Vz/4ePdIu6ZyI/5ZGsYnb+m0JlOmKPjt6XZ9JJkA=" src="https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" /> 
  </head><body>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             accesskey="I">index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">CoderData</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Contribution</a></li> 
      </ul>
    </div>  

    <div class="document">
      <div class="documentwrapper">
        <div class="bodywrapper">
          <div class="body" role="main">
            
  <section id="contribution">
<h1>Contribution<a class="headerlink" href="#contribution" title="Link to this heading">¶</a></h1>
<!-- ---
layout: default
title: CoderData
---

<link rel="stylesheet" href="assets/css/style.css"> -->
<section id="how-to-contribute">
<h2>How to Contribute<a class="headerlink" href="#how-to-contribute" title="Link to this heading">¶</a></h2>
<section id="contribute-to-coderdata">
<h3>Contribute to CoderData<a class="headerlink" href="#contribute-to-coderdata" title="Link to this heading">¶</a></h3>
<p>CoderData is a data assembly pipeline that pulls from
original data sources of drug sensitivity and omics datasets and
assembles them so they can be integrated into a python package for
AI/ML integration.</p>
<p>CoderData is indeed a work in progress. If you have specific requests
or bugs, please file an issue on our <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/issues">GitHub
page</a> and we will
begin a conversation about details and how to fix the issue. If you
would like to create a new feature to address the issue, you are welcome to fork the
repository and create a pull request to discuss it in more
detail. These will be triaged by the CoderData team as they are received.</p>
<p>The rest of this document is focused on how to contribute to and
augment CoderData, either for use by the community or your own
purposes.</p>
<section id="coderdata-build-process">
<h4>CoderData build process<a class="headerlink" href="#coderdata-build-process" title="Link to this heading">¶</a></h4>
<p>To build your own internal Coderdata dataset, or to augment it, it is
important to understand how the package is built.</p>
<p>The build process is managed in the <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/tree/main/build">build
directory</a>
primarily by the <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/build_all.py"><code class="docutils literal notranslate"><span class="pre">build_all.py</span></code> script</a>. The
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/build_dataset.py"><code class="docutils literal notranslate"><span class="pre">build_dataset.py</span></code> script</a> is used for testing the development of each dataset in CoderData.
Because our sample and drug identifiers are unique, we must
finish the generation of one dataset before we move to the next. This
process is depicted below.</p>
<p><code class="docutils literal notranslate"><span class="pre">![Coderdata</span> <span class="pre">Build](coderDataBuild.jpg?raw=true</span> <span class="pre">&quot;Modular</span> <span class="pre">build</span> <span class="pre">process&quot;)</span></code></p>
<p>The build process is slow, partially due to our querying of PubChem,
and also because of our extensive curve fitting. However, it can be
run locally so that you can still leverage the Python package
functionality with your own datasets.</p>
<p>If you want to add a new dataset, you must create a docker image that
contains all the scripts to pull the data and reformat it into our
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/schema/coderdata.yaml">LinkML Schema</a>. Once complete, you can modify <code class="docutils literal notranslate"><span class="pre">build_dataset.py</span></code> to
call your Docker image and associated scripts. More details are below.</p>
</section>
</section>
<section id="adding-your-own-dataset">
<h3>Adding your own dataset<a class="headerlink" href="#adding-your-own-dataset" title="Link to this heading">¶</a></h3>
<p>To add your own data, you must add a Docker image with the following
constraints:</p>
<ol class="arabic simple">
<li><p>Be named <code class="docutils literal notranslate"><span class="pre">Dockerfile.[dataset_name]</span></code> and reside in the
<code class="docutils literal notranslate"><span class="pre">/build/docker</span></code> directory</p></li>
<li><p>Possess scripts called <code class="docutils literal notranslate"><span class="pre">build_omics.sh</span></code>, <code class="docutils literal notranslate"><span class="pre">build_samples.sh</span></code>,
<code class="docutils literal notranslate"><span class="pre">build_drugs.sh</span></code>,  <code class="docutils literal notranslate"><span class="pre">build_exp.sh</span></code> , and if needed, a
<code class="docutils literal notranslate"><span class="pre">build_misc.sh</span></code>. These will all be called directly by
<code class="docutils literal notranslate"><span class="pre">build_dataset.py</span></code>.</p></li>
<li><p>Create tables that mirror the schema described by the <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/schema/coderdata.yaml">LinkML YAML
file</a>.</p></li>
</ol>
<p>Files are generated in the following order as described below.</p>
<section id="sample-generation">
<h4>Sample generation<a class="headerlink" href="#sample-generation" title="Link to this heading">¶</a></h4>
<p>The first step of any dataset build is to create a unique set of
sample identifies and store them in a <code class="docutils literal notranslate"><span class="pre">[dataset_name]_samples.csv</span></code>
file. We recommend following these steps:</p>
<ol class="arabic simple">
<li><p>Build a python script that pulls the sample identifier information
from a stable repository and generates Improve identifiers for
each sample while also ensuring that no sample identifiers are
clashing with prior samples. Examples can be found <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/mpnst/00_sample_gen.R">here</a> and <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/broad_sanger/01-broadSangerSamples.R">here</a>. If
you are using the Genomic Data Commons, you can leverage our
existing scripts <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/hcmi/01-createHCMISamplesFile.py">here</a>.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">build_samples.sh</span></code> script that calls your script with an
existing sample file as the first argument.</p></li>
<li><p>Test the <code class="docutils literal notranslate"><span class="pre">build_samples.sh</span></code> script with a <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/build_test/test_samples.csv">test sample
file</a>.</p></li>
<li><p>Validate the file with the <a class="reference external" href="https://linkml.io/linkml/cli/validate.html">linkML validation tool</a> and our
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/schema/coderdata.yaml">schema file</a>.</p></li>
</ol>
</section>
<section id="omics-data-generation">
<h4>Omics data generation<a class="headerlink" href="#omics-data-generation" title="Link to this heading">¶</a></h4>
<p>The overall omics generation process is the same as the samples, with
a few caveats.</p>
<ol class="arabic simple">
<li><p>Build a python script that maps the omics data and gene data to the
standardized identifiers and aligns them to the schema.
pulls the sample identifier information
from a stable repository and generates Improve identifiers for
each sample while also ensuring that no sample identifiers are
clashing with prior samples. Examples can be found here and here. If
you are using the Genomic Data Commons, you can leverage our
existing scripts here. For each type of omics data (see below), a
single file is created.It might take more than one script, but you
can combine those in step 2.</p></li>
<li><p>Create a <code class="docutils literal notranslate"><span class="pre">build_omics.sh</span></code> script that calls your script with the
<code class="docutils literal notranslate"><span class="pre">genes.csv</span></code> file as the first argument and <code class="docutils literal notranslate"><span class="pre">[dataset_name]_samples.csv</span></code> file as second
argument.</p></li>
<li><p>Test the <code class="docutils literal notranslate"><span class="pre">build_omics.sh</span></code> script with your sample file and <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/build_test/test_genes.csv">test genes
file</a>.</p></li>
<li><p>Validate the files generated with the <a class="reference external" href="https://linkml.io/linkml/cli/validate.html">linkML validation tool</a> and our
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/schema/coderdata.yaml">schema file</a>.</p></li>
</ol>
<p>The precise data files have varying standards, as described below:</p>
<ul class="simple">
<li><p><em>Mutation data:</em> In addition to matching gene identifiers each gene
mutation should be mapped to a specific schema of variations.  The
list of allowed variations can be found <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/8000968dc5f19fbb986a700862c5035a0230b656/schema/coderdata.yaml#L247">in our linkML
file</a>.</p></li>
<li><p><em>Transcriptomic data:</em> Transcript data is mapped to the same gene
identifiers and samples but is convered to transcripts per million,
or TPM.</p></li>
<li><p><em>Copy number data:</em>  Copy number is assumed to be a value
representing the number of copies of that gene in a particular
sample. A value of 2 is assumed to be diploid.</p></li>
<li><p><em>Proteomic data:</em> Proteomic measurements are generally log ratio
values of the abundance measurements normalized to an internal
control.</p></li>
</ul>
<p>The resulting files are then stored as <code class="docutils literal notranslate"><span class="pre">[dataset_name]_[datatype].csv</span></code>.</p>
</section>
<section id="drug-data-generation">
<h4>Drug data generation<a class="headerlink" href="#drug-data-generation" title="Link to this heading">¶</a></h4>
<p>The drug generation process can be slow depending on how many drugs
require querying from PubChem. However, with the use of an existing
drug data file, it’s possible to shorten this process.</p>
<ol class="arabic simple">
<li><p>Build a python script that maps the drug information to SMILES
String and IMPROVE identifier. All drugs are given an Improve
identifier based on the canonical SMILES string to ensure that
each drug has a unique structure to be used in the modeling
process. To standardize this we encourage using
our <a class="reference external" href="http://github.com/pnnl-compbio/coderdata/tree/main/build/utils/pubchem_retrieval.py">standard drug lookup
script</a>
that retrieves drug structure and information by name or
identifier. <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/broad_sanger/03a-nci60Drugs.py">This file of NCI60
drugs</a>
is our most comprehensive script as it pulls over 50k drugs
1a. In cases where the dose and response values are not available, you
can use the published AUC values instead, and use the
<code class="docutils literal notranslate"><span class="pre">published_auc</span></code> as the <code class="docutils literal notranslate"><span class="pre">drug_response_metric</span></code> value in the table.</p></li>
<li><p>Create a  <code class="docutils literal notranslate"><span class="pre">build_drugs.sh</span></code> script that takes as its first argument
an existing drug file and calls the script created in step 1
above. Once the drugs for a dataset are retrieved, we have a second utility
script that <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/cbf017326b83771c55f12317189f4b2dbd9d900a/schema/coderdata.yaml#L94">builds the drug descriptor table</a>. Add this to the
shell script to generate the drug descriptor file.</p></li>
<li><p>Test the <code class="docutils literal notranslate"><span class="pre">build_drugs.sh</span></code> script with the <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/build_test/test_drugs.tsv">test drugs
file</a>.</p></li>
<li><p>Validate the files generated with the <a class="reference external" href="https://linkml.io/linkml/cli/validate.html">linkML validation tool</a> and our
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/schema/coderdata.yaml">schema file</a>.</p></li>
</ol>
<p>The resulting files should be <code class="docutils literal notranslate"><span class="pre">[dataset_name]_drugs.tsv</span></code> and
<code class="docutils literal notranslate"><span class="pre">[dataset_name]_drug_descriptors.tsv</span></code>.</p>
</section>
<section id="experiment-data-generation">
<h4>Experiment data generation<a class="headerlink" href="#experiment-data-generation" title="Link to this heading">¶</a></h4>
<p>The experiment file maps the sample information to the drugs of
interest with various drug response metrics. The experiment data
varies based on the type of system:</p>
<ul class="simple">
<li><p>Cell line and organoid data use the <a class="reference external" href="http://github.com/pnnl-compbio/coderdata/tree/main/build/utils/fit_curve.py">drug curve fitting
tool</a>
that maps doses of drugs (in Moles) to drug response measurements
(in percent) to a variety of curve fitting metrics described in our
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/8000968dc5f19fbb986a700862c5035a0230b656/schema/coderdata.yaml#L200">schema file</a>.</p></li>
<li><p>Patient derived xenografts require an alternate script that <a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/build/utils/calc_pdx_metrics.py">creates
PDX-speciic metrics</a>.</p></li>
</ul>
<p>Otherwise the steps for building an experiment file are similar:</p>
<ol class="arabic simple">
<li><p>Build a python script that maps the drug information and sample
information to the DOSE and GROWTH values, then calls the appropriate
curve fitting tool described above.</p></li>
<li><p>Create a  <code class="docutils literal notranslate"><span class="pre">build_exp.sh</span></code> script that takes as its first argument
the samples file and the second argument the drug file.</p></li>
<li><p>Test the <code class="docutils literal notranslate"><span class="pre">build_exp.sh</span></code> script with the drug and samples files.</p></li>
<li><p>Validate the files generated with the <a class="reference external" href="https://linkml.io/linkml/cli/validate.html">linkML validation tool</a> and our
<a class="reference external" href="https://github.com/PNNL-CompBio/coderdata/blob/main/schema/coderdata.yaml">schema file</a>.</p></li>
</ol>
</section>
<section id="dockerize-and-test">
<h4>Dockerize and test<a class="headerlink" href="#dockerize-and-test" title="Link to this heading">¶</a></h4>
<p>All scripts described above go into a single directory with the name
of the dataset under the <a class="reference external" href="http://github.com/pnnl-compbio/coderdata/tree/main/build">build</a> directory, with instructions to add everything in the <a class="reference external" href="http://github.com/pnnl-compbio/coderdata/tree/main/build/docker">docker</a>
directory. Make sure to include any requirements for building in the
folder and docker image as well.</p>
<p>Once the Dockerfile builds and runs, you can modify the
<code class="docutils literal notranslate"><span class="pre">build_dataset.py</span></code> script so that it runs and validates.</p>
<p>Check out examples! We have numerous Docker files in our
<a class="reference external" href="http://github.com/pnnl-compbio/coderdata/tree/main/build/docker">Dockerfile
directory</a>,
and multiple datasets in our <a class="reference external" href="http://github.com/pnnl-compbio/coderdata/tree/main/build">build
directory</a>.</p>
<hr class="docutils" />
<p>Your contributions are essential to the growth and improvement of CoderData. We look forward to collaborating with you!</p>
</section>
</section>
</section>
<!-- ---
layout: default
title: CoderData
---

<link rel="stylesheet" href="assets/css/style.css"> -->
<section id="guide-to-adding-code-to-coderdata">
<h2>Guide to Adding Code to CoderData<a class="headerlink" href="#guide-to-adding-code-to-coderdata" title="Link to this heading">¶</a></h2>
<section id="introduction">
<h3>Introduction<a class="headerlink" href="#introduction" title="Link to this heading">¶</a></h3>
<p>This guide outlines the steps for contributors looking to add new code functionalities to CoderData, including automation scripts, Docker integration, and continuous integration processes.</p>
</section>
<section id="automate-credentials-and-data-pulling">
<h3>1. Automate Credentials and Data Pulling<a class="headerlink" href="#automate-credentials-and-data-pulling" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Script Development</strong></em>: Write a script to automate credential management and data pulling, ensuring secure handling of API keys or authentication tokens.</p></li>
<li><p><em><strong>Data Formatting</strong></em>: The script should reformat the pulled data to fit CoderData’s existing schema.</p></li>
<li><p><em><strong>Environment Variables</strong></em>: Store sensitive information such as API keys in environment variables for security.</p></li>
</ul>
</section>
<section id="writing-tests">
<h3>2. Writing Tests<a class="headerlink" href="#writing-tests" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Unit Tests</strong></em>: Create unit tests for each function in your script.</p></li>
<li><p><em><strong>Data Tests</strong></em>: Implement data tests to verify your data is accessing APIs correctly, properly formatted, and matches the schema.</p></li>
</ul>
</section>
<section id="dockerization">
<h3>3. Dockerization<a class="headerlink" href="#dockerization" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Dockerfile Creation</strong></em>: Write a Dockerfile to containerize your script, specifying dependencies, environment setup, and entry points.</p></li>
<li><p><em><strong>Local Testing</strong></em>: Test the Docker container locally to confirm correct functionality.</p></li>
</ul>
</section>
<section id="github-actions-for-automation">
<h3>4. GitHub Actions for Automation<a class="headerlink" href="#github-actions-for-automation" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Workflow Setup</strong></em>: Design a GitHub Actions workflow to automate the script execution. You may directly update our version, or create a seperate workflow and we could join it to ours.  This should include:</p>
<ul>
<li><p>Trigger mechanisms (schedule or events).</p></li>
<li><p>Secret management for credentials.</p></li>
<li><p>Error logging and handling.</p></li>
<li><p>Seperate steps for samples file generation and the rest of the data generation.</p></li>
</ul>
</li>
</ul>
</section>
<section id="updating-documentation">
<h3>5. Updating Documentation<a class="headerlink" href="#updating-documentation" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Documentation Revision</strong></em>: Update the project documentation to reflect your new script’s purpose and usage.</p></li>
<li><p><em><strong>Usage Examples</strong></em>: Provide clear examples and usage instructions.</p></li>
</ul>
</section>
<section id="continuous-integration">
<h3>6. Continuous Integration<a class="headerlink" href="#continuous-integration" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>CI Integration</strong></em>: Integrate your script into the existing Continuous Integration pipeline.</p></li>
<li><p><em><strong>CI Testing</strong></em>: Ensure your script is included in the CI testing process to prevent future breakages.</p></li>
</ul>
</section>
<section id="pull-request-and-code-review">
<h3>7. Pull Request and Code Review<a class="headerlink" href="#pull-request-and-code-review" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Create a Pull Request</strong></em>: Submit your changes via a pull request to the main CoderData repository.</p></li>
</ul>
</section>
<section id="align-with-repository-standards">
<h3>8. Align with Repository Standards<a class="headerlink" href="#align-with-repository-standards" title="Link to this heading">¶</a></h3>
<ul class="simple">
<li><p><em><strong>Review Project Guidelines</strong></em>: Familiarize yourself with the CoderData project’s standards and practices to align your contribution accordingly.</p></li>
</ul>
</section>
<section id="conclusion">
<h3>Conclusion<a class="headerlink" href="#conclusion" title="Link to this heading">¶</a></h3>
<p>Contributors are encouraged to follow these guidelines closely for effective integration of new code into the CoderData project. We appreciate your contributions and look forward to your innovative enhancements to the project.</p>
<hr class="docutils" />
<p>Your contributions are essential to the growth and improvement of CoderData. We look forward to collaborating with you!</p>
</section>
</section>
</section>


            <div class="clearer"></div>
          </div>
        </div>
      </div>
      <div class="sphinxsidebar" role="navigation" aria-label="Main">
        <div class="sphinxsidebarwrapper">
            <p class="logo"><a href="index.html">
              <img class="logo" src="_static/coderdata_logo3.png" alt="Logo of CoderData"/>
            </a></p><div  class="sphinxdoc-sidebar">
    <h3>Resource Links</h3>
    <ul>
        <li><a href="https://github.com/PNNL-CompBio/coderdata/" target="_blank">Github</a></li>
        <li><a href="https://pypi.org/project/coderdata/" target="_blank">PYPI</a></li>
        <li><a href="https://figshare.com/articles/dataset/CODERData2_1_0/28823159" target="_blank">FigShare</a></li>
    </ul>
</div>

<div class="sphinxdoc-sidebar">
    <h3>Quick Access</h3>
    <ul>
        <li><a href="index.html">Home</a></li>
        <li><a href="usage.html">Usage</a></li>
        <li><a href="APIreference.html">API Reference</a></li>
        <li><a href="datasets_included.html">Datasets Included</a></li>
        <li><a href="tutorials.html">Tutorials</a></li>
        <li><a href="#">How to Contribute</a></li>
    </ul>
</div>


<!-- <div class="sphinxdoc-sidebar"> -->
    <!-- Display Current Page Name -->
    <!-- <h3>Current Page</h3> -->
    <!-- <p>contribution_guide</p> -->

    <!-- Previous/Next Page Navigation -->
    <!-- <h3>Navigation</h3> -->
    <!-- <ul>
         -->
        
        <!--  -->
    <!-- </ul> -->
<!-- </div>    -->
  <div>
    <h3><a href="index.html">Table of Contents</a></h3>
    <ul>
<li><a class="reference internal" href="#">Contribution</a><ul>
<li><a class="reference internal" href="#how-to-contribute">How to Contribute</a><ul>
<li><a class="reference internal" href="#contribute-to-coderdata">Contribute to CoderData</a><ul>
<li><a class="reference internal" href="#coderdata-build-process">CoderData build process</a></li>
</ul>
</li>
<li><a class="reference internal" href="#adding-your-own-dataset">Adding your own dataset</a><ul>
<li><a class="reference internal" href="#sample-generation">Sample generation</a></li>
<li><a class="reference internal" href="#omics-data-generation">Omics data generation</a></li>
<li><a class="reference internal" href="#drug-data-generation">Drug data generation</a></li>
<li><a class="reference internal" href="#experiment-data-generation">Experiment data generation</a></li>
<li><a class="reference internal" href="#dockerize-and-test">Dockerize and test</a></li>
</ul>
</li>
</ul>
</li>
<li><a class="reference internal" href="#guide-to-adding-code-to-coderdata">Guide to Adding Code to CoderData</a><ul>
<li><a class="reference internal" href="#introduction">Introduction</a></li>
<li><a class="reference internal" href="#automate-credentials-and-data-pulling">1. Automate Credentials and Data Pulling</a></li>
<li><a class="reference internal" href="#writing-tests">2. Writing Tests</a></li>
<li><a class="reference internal" href="#dockerization">3. Dockerization</a></li>
<li><a class="reference internal" href="#github-actions-for-automation">4. GitHub Actions for Automation</a></li>
<li><a class="reference internal" href="#updating-documentation">5. Updating Documentation</a></li>
<li><a class="reference internal" href="#continuous-integration">6. Continuous Integration</a></li>
<li><a class="reference internal" href="#pull-request-and-code-review">7. Pull Request and Code Review</a></li>
<li><a class="reference internal" href="#align-with-repository-standards">8. Align with Repository Standards</a></li>
<li><a class="reference internal" href="#conclusion">Conclusion</a></li>
</ul>
</li>
</ul>
</li>
</ul>

  </div>
<search id="searchbox" style="display: none" role="search">
  <h3 id="searchlabel">Quick search</h3>
    <div class="searchformwrapper">
    <form class="search" action="search.html" method="get">
      <input type="text" name="q" aria-labelledby="searchlabel" autocomplete="off" autocorrect="off" autocapitalize="off" spellcheck="false"/>
      <input type="submit" value="Go" />
    </form>
    </div>
</search>
<script>document.getElementById('searchbox').style.display = "block"</script>
        </div>
      </div>
      <div class="clearer"></div>
    </div>
    <div class="related" role="navigation" aria-label="Related">
      <h3>Navigation</h3>
      <ul>
        <li class="right" style="margin-right: 10px">
          <a href="genindex.html" title="General Index"
             >index</a></li>
        <li class="right" >
          <a href="py-modindex.html" title="Python Module Index"
             >modules</a> |</li>
        <li class="nav-item nav-item-0"><a href="index.html">CoderData</a> &#187;</li>
        <li class="nav-item nav-item-this"><a href="">Contribution</a></li> 
      </ul>
    </div>
    <div class="footer" role="contentinfo">
    &#169; Copyright 2025, Sara Gosline.
      Created using <a href="https://www.sphinx-doc.org/">Sphinx</a> 8.1.3.
    </div>
  </body>
</html>