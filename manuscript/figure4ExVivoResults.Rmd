---
title: "Figure 4 ex vivo results"
author: "Sara Gosline and Yannick Mahlich"
date: "2025-06-02"
output: html_document
---

This figure focuses on the ex vivo results

## Data and packages

First we get the packages loaded and logged into synapse.
```{r setup, include = FALSE}
knitr::opts_chunk$set(echo  =  TRUE)
library(tidyverse)
library(ggplot2)
library(arrow)
library(dplyr)
library(gridExtra)
source('coderdataResultsFunctions.R')
```


The data has been uploaded by natasha and can be downloaded as follows.

```{r download data, eval}

  cdres <- getModelPerformanceData()

  ecdres <- subset(cdres,trg %in% exvivo)


```

## Figure 4A


```{r, eval=FALSE}
metrics <- c('scc','pcc')
exres = lapply(metrics,function(x) {
  res<-ridgelineMetricPlots(x, ecdres,'cellline')
  cowplot::plot_grid(res$src,res$trg)
  ggsave(paste0('exvivo',x,'_ridglines.pdf'),height = 8,width = 10)
  return(res$src)
  })

print(exres)
```

Now we can confirm that the datasets follow the same pattern. 

```{r dataset samples, eval=FALSE}


plot<-calcSourceStatistics('scc',ecdres)


ggsave('exVivoSamplePerformanceCorrelation.pdf',plot, height=12,width=5)
print(plot)

```

## AUC Study

Does true AUC correlate with how well an algorithm works?

### Multipanel correlation plots

First we evaluate how this hypothesis bears out in CCLE predictions.

Full model predictions are stored on synapse as parquet files. Individual
datasets can be downloaded via `getModelPredictionData` in
`coderdataResultsFunctions.R` (sources during the setup process).

```{r}
tgt = 'ccle'

all_preds <- do.call(
  rbind,
  lapply(
    models,
    function(mdl) getModelPredictionData(dset = mdl) |>
      dplyr::filter(target == tgt & source != tgt & source != 'beataml' & source != 'mpnst') |>
      #dplyr::filter(source != tgt & source != 'beataml' & source != 'mpnst') |>
      collect()
    )
  )
```


```{r}
plot_panel <- function(data, title){
  data <- sample_n(data, 10000)
  plot <- (
    ggplot(data, aes(x=auc_pred, y=auc_true))
    + geom_point()
    + geom_smooth(method=lm)
    + facet_grid(source ~ model)
    + ggtitle(title)
    # + xlim(0, 1)
    # + ylim(0, 1.25)
  )
}
```

We add more statistics to the data here

```{r}
all_preds <- all_preds |> 
  mutate(auc_ranges = cut(auc_true, c(-Inf, 0.25, 0.75, Inf), labels = c('auc_true <= 0.25', '0.25 < auc_true <= 0.75', 'auc_true > 0.75'))) |>
  mutate(diff = abs(auc_true - auc_pred)) |>
  mutate(norm_diff = diff/auc_true)

```



```{r}
ranges <- list('auc_true <= 0.25', '0.25 < auc_true <= 0.75', 'auc_true > 0.75')
plots <- lapply(ranges, function(auc_range){
  data <- all_preds |> filter(auc_ranges == auc_range) |> collect()
  plot_panel(data, auc_range)
})
plot <- arrangeGrob(grobs = plots, ncol = 3)
ggsave('ccle_auc_plot.pdf', plot, dpi=300, width=30, height=10)
```

### Summarize error and plot across all data

First lets see the over all distribution
```{r error summary ploting}

ggplot(all_preds,aes(x=norm_diff,fill=model))+geom_histogram()+facet_grid(source~auc_ranges)+scale_y_log10()+scale_fill_manual(values=modelcolors)
```

This is still too much data, since it only represents CCLE preditions, let's try to compute summaries independetly.

```{r summaries}

all_summaries <- do.call(
  rbind,
  lapply(
    models,
    function(mdl) getModelPredictionData(dset = mdl) |>
      dplyr::select(auc_true,auc_pred,source,target,model) |> ##remove columsn to consume less memory
      dplyr::filter(source != 'beataml' & source != 'mpnst') |>
      dplyr::filter(!target %in% c('beataml','sarcpdo','pancpdo','mpnst','bladderpdo')) |>
      #dplyr::filter(source != tgt & source != 'beataml' & source != 'mpnst') |>
      collect() |>
      mutate(auc_ranges = cut(auc_true, c(-Inf, 0.25, 0.75, Inf), labels = c('auc_true <= 0.25', '0.25 < auc_true <= 0.75', 'auc_true > 0.75'))) |>
      mutate(diff = abs(auc_true - auc_pred)) |>
      mutate(norm_diff = diff/auc_true) |>
      group_by(source, target, auc_ranges,model) |>
      summarize(`Median Difference` = median(diff))
    )
  )

all_summaries |>
  ggplot(aes(x = auc_ranges,y = `Median Difference`,fill = model)) + 
  geom_bar(position = 'dodge',stat = 'identity') + 
  facet_grid(target~source) + 
  coord_flip() + 
  scale_fill_manual(values=modelcolors)

ggsave('medianDifferenceCellLine.pdf')

```


## Drug panel

Now let's look across different drugs. Filter by drugs that show up in all models/datasets and then evaluate which perform best/worst.

```{r drugs}

```

## MPNST test

Here we download data from all models, subset to only MPNST target predictions
and combine the individual subsets into one master table.
```{r predictions data import}

tgt = 'mpnst'

all_preds <- do.call(
  rbind,
  lapply(
    models,
    function(mdl) getModelPredictionData(dset = mdl) |>
      dplyr::filter(target == tgt & source != tgt & source != 'beataml') |>
      collect()
    )
  )

```

We want to group results by drugs i.e. create a panel per drug. To that end we 
extract the drugs and determine the "grid layout" by size of target drugs.
```{r}

drugs <- unique(all_preds$improve_chem_id)
grid_ncol = 4
grid_nrow = ceiling(length(drugs) / grid_ncol)
```

Defining the plot function for the individual "panels"
```{r}
plot_panel <- function(data, title){
  plot <- (
    ggplot(data, aes(x=auc_true, y=auc_pred))
    + geom_point()
    + geom_smooth(method=lm)
    + facet_grid(source ~ model)
    + ggtitle(title)
    + xlim(0, 1)
    + ylim(0, 1.25)
  )
}

```

```{r}
plots <- lapply(drugs, function(drug_id){
  data <- all_preds |> filter(improve_chem_id == drug_id) |> collect()
  plot_panel(data, drug_id)
})
```

```{r}
plot <- arrangeGrob(grobs = plots, ncol = grid_ncol, nrow = grid_nrow)
```


```{r}
# print(plot)
ggsave('mpnst_auc_plot.pdf', plot, dpi=300, width=40, height=60, limitsize = FALSE)
```
